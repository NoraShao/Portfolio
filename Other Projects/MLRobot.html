<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="description" content="Portfolio" />
    <link rel="preconnect" href="https://fonts.googleapis.com" />
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
    <link
      href="https://fonts.googleapis.com/css2?family=Poppins:wght@400;500;700&display=swap"
      rel="stylesheet"
    />
    <script
      src="https://kit.fontawesome.com/fa62c117c7.js"
      crossorigin="anonymous"
    ></script>
    <link rel="stylesheet" href="../styles.css" />
    <title>Nora Shao</title>
  </head>


  <body class = "light" id = "top">
    <header class="header center">
      <h3>
        <a >Course Projects and More</a
        >
      </h3>
  
      <nav class="nav center">
        <ul class="nav__list center">
          <li class="nav__list-item">
            <a class="link link--nav" href="../index.html">Home</a>
          </li>
        </ul>

        <button type="button" aria-label="toggle theme" class="btn btn--icon">
            <i aria-hidden="true" id="btn-theme" class="fas fa-moon"></i>
        </button>
      </nav>

      
    </header>
  
    <main>
      <section class="section__peripheral">
      <h2 class = "section__title">Machine Learning Racing Robot</h2>

        
        <div class="project__peripheral">
          <p class="project__subheading"> Course: ENPH 353 (Machine Learning) | 
              September 2024 - December 2024</p>
          <p class="project__description"><b>Project Description:</b> In this class, we built up knowledge and skills related
          to various software applications like Linux and machine learning via labs to ultimately design an autonomous virtual
          robot to compete in a simulated competition. </p>
          <br>
          <br>

          <h3>Line Detection from Video</h3>
          <div class="project-container">
            <div class="project-text">
              <p>
                In this lab, we were given a video of a path, and assigned the task of generating the same video, but with
                a ball following the path. <br>
                I processed the video frame-by-frame with OpenCV, and started by converting each frame to grayscale.
              </p>
            </div>
          </div>
          <section class="project-container" style="display: flex; align-items: flex-start;">
            <div class="project-text" style="align-self: flex-start;">
              <p>
                Then I thresholded the video so the darker path was the only black in the video. To determine the correct
                threshold that would differentiate between the intensity of path and its surroundings, I took a ten pixel-high
                horizontal section near the bottom of the frame and plotted the mean pixel intensities across it. 
              </p>  
            </div>
            <div style="display: flex; align-items: flex-start; gap: 20px;">
              <figure>
                <img src="Other Media/ENPH353_Lab2_PixelIntensity.png" alt="Pixel Intensities" 
                class="project-image__img" style="width: 350px; height: auto; border: 2px solid #e0e0e0; 
                box-shadow: 2px 2px 8px rgba(0, 0, 0, 0.1); margin-top: -10px;" />
              <figcaption>Average pixel intensity across frame from row 220-230</figcaption>
            </figure>
            <figure>
              <div class="video-container">
                <iframe
                  src="https://drive.google.com/file/d/1xl7Pnjv121lf8JLc6t3PqntfMurXFqho/preview"
                  width="100%"
                  height="300"
                  style="max-width: 400px; margin: 0 auto; display: block; border: none;"
                  allow="autoplay"
                ></iframe>
              </div>
              <!-- <video controls="controls" width="100%" style="max-width: 400px; margin: 0 auto; display: block;">
                <source src="https://NoraShao.github.io/Portfolio/Other Projects/Other Media/out_with_ball.mp4" type="video/mp4">
              </video> -->
                
                <figcaption style="text-align: center; margin-top: 10px; font-size: 0.9em; color: var(--clr-fg-alt);">
                  Thresholded black-and-white video.
                </figcaption>
            </figure>
              
            </div>
            
          </section>

            
            
          
          <section class="project-container" style="display: flex; align-items: flex-start;">
            <div class="project-text">
              For simplicity, I decided to center the ball around a constant horizontal slice of pixels near the bottom of the
              frame. With the frames from the thresholded video as a reference for where to overlay a ball on the path in the 
              original video, I found the horizontal coordinate for the ball's center by randomly generating a location in the section
              of black pixels (which I knew identified the path). I drew the ball and overlayed the image on the original 
              coloured video with OpenCV.
            </div>

            
            
          </section>

          <div style="display: flex; align-items: flex-start; gap: 20px;">
            <figure>
              <div class="video-container">
                <iframe
                  src="https://drive.google.com/file/d/1c6UId9nUNo9ve6Ad9DFFpt95JE4HqSyo/preview"
                  width="100%"
                  height="300"
                  style="max-width: 400px; margin: 0 auto; display: block; border: none;"
                  allow="autoplay"
                ></iframe>
              </div>
                <figcaption style="text-align: center; margin-top: 10px; font-size: 0.9em; color: var(--clr-fg-alt);">
                  Raw video feed
                </figcaption>
            </figure>
           
            <figure>
            <div class="video-container">
              <iframe
                src="https://drive.google.com/file/d/1--WI4CMUI2pFgdOwgUuRnWlLD3sZ_sI8/preview"
                width="100%"
                height="300"
                style="max-width: 400px; margin: 0 auto; display: block; border: none;"
                allow="autoplay"
              ></iframe>
            </div>
              <figcaption style="text-align: center; margin-top: 10px; font-size: 0.9em; color: var(--clr-fg-alt);">
                Output video with path-following ball
              </figcaption>
          </figure>
 
          </div>
          

          <div class="project__stack__peripheral">
            <span class="project__stack-item">OpenCV</span>
            <span class="project__stack-item">Python</span>
          </div>
          <a href="#" class="link">View on GitHub</a>



          <br>
          <br>
          <h3>License plate recognizer (Convolutional Neural Network)</h3>
          <section class="project-container">
            <div class="project-text">
              <p>
                In this lab, I trained a neural network to recognize characters off generated license
                plate images.
              </p>

              
            </div>
            
          </section>
          <br>
          <br>


          <h3>GUI and Image Tracking with SIFT</h3>
          <section class="project-container" style="display: flex; align-items: flex-start;">
            <div class="project-text" style="align-self: flex-start;">
            <p>
              Moving back to the course's Linux environment, I set up a 'SIFT App' program that could recognize and track an
              image via SIFT from a video and an accompanying GUI with PyQt5. The program ran a SIFT algorithm on the video
              to identify keypoints, then uses a FLANN algorithm to identify and draw matches between these points on the
              template image and video. Finally, if enough 'good' matches are found, a blue square identifying the frame of
              the template image is drawn over the video.
            </p>
            <figure>
              <div class="video-container">
                <iframe
                  src="https://drive.google.com/file/d/1dpt-xLdRu8IOYYmyqsXJKSF0GIZFUYFv/preview"
                  width="100%"
                  height="300"
                  style="max-width: 1000px; margin: 0 auto; display: block; border: none;"
                  allow="autoplay"
                ></iframe>
              </div>
                <figcaption style="text-align: center; margin-top: 10px; font-size: 0.9em; color: var(--clr-fg-alt);">
                  Demonstration of SIFT App working in the GUI
                </figcaption>
            </figure>
            
           </div>
          </section>

          <section class="project-container" style="display: flex; align-items: flex-start;">
            <div class="project-text" style="align-self: flex-start;">
             <p>
               The first thing I made was the GUI, which was done in Qt Designer. For modularity, I set up options for the template image and video to draw
               the feature matches between. 
             </p>
             <div style="display: flex; align-items: flex-start; gap: 20px;">
              <figure>
                <img src="Other Media/ENPH353_Lab4_GUI.png" alt="SIFT GUI" 
                class="project-image__img" style="width: 1000px; height: auto; border: 2px solid #e0e0e0; 
                box-shadow: 2px 2px 8px rgba(0, 0, 0, 0.1); margin-top: -10px;" />
              <figcaption>SIFT App GUI building with PyQt5</figcaption>
              </figure>
             </div>
              
            </div>
          </section>
          <section class="project-container">
            <div class="project-text">
              
              
              SIFT, Scale-invariant feature transform, is a computer vision algorithm that detects distinctive features in an
              image that are invariant to transformations like scaling and rotation, or viewpoint changes, making it useful
              for object detection, which is what I'm using it for in this lab. For tracking the template image in the video,
              I started by converting each frame of the video to grayscale. Then I used OpenCV's SIFT function to detect
              keypoints on and compute descriptors of each frame of the video, and draw them on the frame.
               
              <br>
              To draw the matches between the keypoints on the template image and video, I used FLANN (Fast Library for 
              Approximate Nearest Neighbors). Using k nearest-neighbor matching (kNN) with the two closest matches to a
              keypoint descriptor, FLANN lets me quickly identify good matches between the template image and video frame.
            </div>
            <div style="display: flex; flex-direction: column; align-items: flex-start; gap: 20px;">
              <figure>
                <img src="Other Media/VirtualBox_ENPH353_Xubuntu_2023_01_10_2024_16_23_42.png" alt="SIFT GUI" 
                class="project-image__img" style="width: 1000px; height: auto; border: 2px solid #e0e0e0; 
                box-shadow: 2px 2px 8px rgba(0, 0, 0, 0.1); margin-top: -10px;" />
              <figcaption>SIFT keypoints on the video</figcaption>
              </figure>
              <br>
              <figure>
                <img src="Other Media/VirtualBox_ENPH353_Xubuntu_2023_01_10_2024_19_40_26.png" alt="SIFT GUI" 
                class="project-image__img" style="width: 1000px; height: auto; border: 2px solid #e0e0e0; 
                box-shadow: 2px 2px 8px rgba(0, 0, 0, 0.1); margin-top: -10px;" />
              <figcaption>Keypoint matching between template image and video</figcaption>
              </figure>
             </div>
          </section>
          <section class="project-container">
            <div class="project-text">
              To track the distorted frame of the image in the video, I used a homography, which is a mathematical 
              transform that relates the position of points between images taken from different perspectives. If more than ten
              'good' keypoints are found, then a homography mask is drawn over the video. To get rid of bad outlier matches, I used
              RANSAC (Random Sample Consensus), which estimates a homography using a random subset of matches and checks how
              many of the other matches fit in with the homography. The process is repeated several times. See the 
            </div>
          </section>

          <div class="project__stack__peripheral">
            <span class="project__stack-item">SIFT</span>
            <span class="project__stack-item">FLANN</span>
            <span class="project__stack-item">RANSAC</span>
            <span class="project__stack-item">OpenCV</span>
            <span class="project__stack-item">PyQt5</span>

          </div>
          <a href="https://github.com/NoraShao/ENPH353_Lab4.git" class="link">View on GitHub</a>
          
          
          <br>
          <br>
          <h3>License plate recognizer (Convolutional Neural Network)</h3>
          <section class="project-container">
            <p>
              Here, I trained a convolutional neural network (CNN) on Google colab to recognize characters from 
              generated license plate images.
            </p>
          </section>
          <section class="project-container" style="display: flex; align-items: flex-start;">
            
            <div class="project-text" style="align-self: flex-start;">
              We were provided with a license plate generator that randomixed two numerical characters following two
              alphabetical characters on a license plate template with fixed spacing.
            </div>
            <figure>
              <img src="Other Media/plate_AY43.png" alt="License Plate AY43" 
              class="project-image__img" style="width: 500px; height: auto; border: 2px solid #e0e0e0; 
              box-shadow: 2px 2px 8px rgba(0, 0, 0, 0.1); margin-top: -10px;" />
              <figcaption>Example of generated license plate</figcaption>
            </figure>
            <br>
            <div class="project-text" style="align-self: flex-start;">
              <p>
                Before I started working on character recognition with the CNN, I had to start with some image processing.
                Taking advantage of the fixed spacing in all the license plates, I generated subsections of the license plates
                with each individual character.
              </p>
            </div>
            <figure>
              <img src="Other Media/plate_AY43_char 3.png" alt="License Plate AY43" 
              class="project-image__img" style="width: 500px; height: auto; border: 2px solid #e0e0e0; 
              box-shadow: 2px 2px 8px rgba(0, 0, 0, 0.1); margin-top: -10px;" />
              <figcaption>Character 3 of license plate AY43</figcaption>
            </figure>
          </section>
            



          <div class="project__stack__peripheral">
            <span class="project__stack-item">C/C++</span>
            <span class="project__stack-item">STMCubeIDE</span>
          </div>
          <a href="#" class="link">View on GitHub</a>
        </div>



        <div class="project__stack__peripheral">
            <span class="project__stack-item">Imitation Learning</span>
            <span class="project__stack-item">Gazebo</span>
            <span class="project__stack-item">Solidworks</span>
        </div>
      
      </section>
    </main>
  
    <footer class="footer">
      <p>&copy; 2024 My Portfolio</p>
    </footer>

    <div class="scroll-container">
        <div class="scroll-top">
          <a aria-label="Scroll up" href="#top">
            <i aria-hidden="true" class="fas fa-arrow-up"></i>
          </a>
        </div>
      </div>
  
      <script src="../script.js"></script>


  </body>
</html>
